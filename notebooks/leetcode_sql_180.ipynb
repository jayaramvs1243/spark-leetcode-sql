{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81958b80-4410-4851-a4d1-08778664067a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### LeetCode - SQL - #180\n",
    "\n",
    "Refer [180. Consecutive Numbers](https://leetcode.com/problems/consecutive-numbers/description/)\n",
    "\n",
    "Find all numbers that appear at least three times consecutively.\n",
    "\n",
    "Return the result table in  **any order**."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_numbers = [[1, 1], [2, 1], [3, 1], [4, 2], [5, 1], [6, 2], [7, 2]]\n",
    "columns_numbers = ['id', 'num']\n",
    "schema_numbers = {'id':'Int64', 'num':'Int64'}"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "pandas_df_numbers = pandas.DataFrame(data=data_numbers, columns=columns_numbers).astype(schema_numbers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_context = SparkSession.builder.appName(\"LeetCode SQL\").getOrCreate()\n",
    "\n",
    "# Spark DataFrame using Pandas DataFrame\n",
    "df_numbers = spark_context.createDataFrame(pandas_df_numbers)\n",
    "df_numbers.printSchema()\n",
    "df_numbers.show(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Using Window Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "window_spec = Window.orderBy(df_numbers.id)\n",
    "df_numbers = \\\n",
    "    df_numbers \\\n",
    "        .withColumn(\"lag_num\", lag(df_numbers.num, 1).over(window_spec)) \\\n",
    "        .withColumn(\"lead_num\", lead(df_numbers.num, 1).over(window_spec))\n",
    "df_numbers.show(5)\n",
    "\n",
    "df_numbers \\\n",
    "    .filter((df_numbers.num == df_numbers.lag_num) & (df_numbers.num == df_numbers.lead_num)) \\\n",
    "    .select(df_numbers.num.alias(\"ConsecutiveNums\")) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Using SQL Joins"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_root = df_numbers.alias(\"number\")\n",
    "df_lag = df_numbers.alias(\"lag\")\n",
    "df_lead = df_numbers.alias(\"lead\")\n",
    "\n",
    "df_joined = \\\n",
    "    df_root.join(df_lag, (col(\"number.id\") - 1) == col(\"lag.id\"), \"inner\") \\\n",
    "        .join(df_lead, (col(\"number.id\") + 1) == col(\"lead.id\"), \"inner\") \\\n",
    "        .select(col(\"lag.num\").alias(\"num_lag\"), col(\"number.num\").alias(\"num\"), col(\"lead.num\").alias(\"num_lead\"))\n",
    "\n",
    "df_joined \\\n",
    "    .filter((df_joined.num_lag == df_joined.num) & (df_joined.num == df_joined.num_lead)) \\\n",
    "    .select(df_joined.num.alias(\"ConsecutiveNums\")) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "leetcode_sql_180",
   "widgets": {}
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
