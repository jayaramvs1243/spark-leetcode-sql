{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0d6ead-49b4-4222-a90f-02d2924f57fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### LeetCode - SQL - #1050\n",
    "\n",
    "Refer [1050. Actors and Directors Who Cooperated At Least Three Times](https://leetcode.com/problems/actors-and-directors-who-cooperated-at-least-three-times/description/)\n",
    "\n",
    "Write a solution to find all the pairs  `(actor_id, director_id)`  where the actor has cooperated with the director at least three times.\n",
    "\n",
    "Return the result table in  **any order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662acd67-0f1b-4e04-b587-7960bad2ca67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_actor_director = [[1, 1, 0], [1, 1, 1], [1, 1, 2], [1, 2, 3], [1, 2, 4], [2, 1, 5], [2, 1, 6]]\n",
    "columns_actor_director = ['actor_id', 'director_id', 'timestamp']\n",
    "schema_actor_director = {'actor_id':'int64', 'director_id':'int64', 'timestamp':'int64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defeefbb-051b-44e1-896d-fa98105b0c4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_context = SparkSession.builder.appName('LeetCode SQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c8cc71-64b4-4365-ae6a-a4cda900bb87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe using Pandas\n",
    "import pandas\n",
    "\n",
    "pandas_df_actor_director = pandas.DataFrame(data=data_actor_director, columns=columns_actor_director).astype(schema_actor_director)\n",
    "\n",
    "df_actor_director = spark_context.createDataFrame(pandas_df_actor_director)\n",
    "df_actor_director.printSchema()\n",
    "df_actor_director.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e46d1b8-3f12-4c9e-9ee2-1ad91cfe107b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe using PySpark functions - 1\n",
    "schema_actor_director = 'actor_id integer, director_id integer, timestamp integer'\n",
    "\n",
    "df_actor_director = spark_context.createDataFrame(data=data_actor_director, schema=schema_actor_director)\n",
    "df_actor_director.printSchema()\n",
    "df_actor_director.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02860414-496a-4bcd-a163-2559b8139c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe using PySpark functions - 2\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "schema_actor_director = StructType([\n",
    "    StructField('actor_id', IntegerType(), False),\n",
    "    StructField('director_id', IntegerType(), False),\n",
    "    StructField('timestamp', IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_actor_director = spark_context.createDataFrame(data=data_actor_director, schema=schema_actor_director)\n",
    "df_actor_director.printSchema()\n",
    "df_actor_director.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5638ae5f-9c21-402f-8881-e4635efdb44b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|actor_id|director_id|\n+--------+-----------+\n|       1|          1|\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_actor_director_grouped \\\n",
    "    = df_actor_director \\\n",
    "        .groupBy(df_actor_director.actor_id, df_actor_director.director_id).agg(\n",
    "            F.count('*').alias('cooperated_count')\n",
    "        )\n",
    "\n",
    "df_actor_director_grouped \\\n",
    "    .filter(df_actor_director_grouped.cooperated_count >= 3) \\\n",
    "    .select(df_actor_director_grouped.actor_id, df_actor_director_grouped.director_id) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "leetcode_sql_1050",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
