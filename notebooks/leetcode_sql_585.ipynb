{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5245fd8-80e3-49e1-ab4e-cb75b6c63fb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### LeetCode - SQL - #585\n",
    "\n",
    "Refer [585. Investments in 2016](https://leetcode.com/problems/investments-in-2016/description/)\n",
    "\n",
    "Write a solution to report the sum of all total investment values in 2016  `tiv_2016`, for all policyholders who:\n",
    "\n",
    "-   have the same  `tiv_2015`  value as one or more other policyholders, and\n",
    "-   are not located in the same city as any other policyholder (i.e., the (`lat, lon`) attribute pairs must be unique).\n",
    "\n",
    "Round  `tiv_2016`  to  **two decimal places**."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_insurance = [[1, 10, 5, 10, 10], [2, 20, 20, 20, 20], [3, 10, 30, 20, 20], [4, 10, 40, 40, 40]]\n",
    "columns_insurance = ['pid', 'tiv_2015', 'tiv_2016', 'lat', 'lon']\n",
    "schema_insurance = {'pid':'Int64', 'tiv_2015':'Float64', 'tiv_2016':'Float64', 'lat':'Float64', 'lon':'Float64'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "pandas_dataframe_insurance = pandas.DataFrame(data=data_insurance, columns=columns_insurance).astype(schema_insurance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_context = SparkSession.builder.appName(\"LeetCode SQL\").getOrCreate()\n",
    "\n",
    "# Spark DataFrame using Pandas DataFrame\n",
    "dataframe_insurance = spark_context.createDataFrame(pandas_dataframe_insurance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "window_spec_tiv_2015 = Window.partitionBy(dataframe_insurance.tiv_2015)\n",
    "window_spec_lat_lan = Window.partitionBy(dataframe_insurance.lat, dataframe_insurance.lon)\n",
    "\n",
    "dataframe_insurance \\\n",
    "    = dataframe_insurance \\\n",
    "        .withColumn(\"tiv_2015_count\", F.count(\"*\").over(window_spec_tiv_2015)) \\\n",
    "        .withColumn(\"lat_lan_count\", F.count(\"*\").over(window_spec_lat_lan))\n",
    "dataframe_insurance.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tiv_2016|\n",
      "+--------+\n",
      "|    45.0|\n",
      "+--------+\n"
     ]
    }
   ],
   "source": [
    "dataframe_insurance \\\n",
    "    .filter((dataframe_insurance.tiv_2015_count > 1) & (dataframe_insurance.lat_lan_count == 1)) \\\n",
    "    .select(F.round(F.sum(dataframe_insurance.tiv_2016), 2).alias(\"tiv_2016\")) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T17:40:38.518130Z",
     "start_time": "2024-03-10T17:40:38.284680Z"
    }
   },
   "execution_count": 11
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "leetcode_sql_585",
   "widgets": {}
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
